{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackysiupuichung/mlops_zoomcamp/blob/main/amazon_product_search_Recommendation_Engines_mlflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# === 🔧 Install Missing Packages ===\n",
        "def ensure_packages():\n",
        "    required = {'sentence_transformers', 'annoy', 'mlflow', 'pytorch-lightning'}\n",
        "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "    missing = required - installed\n",
        "    if missing:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
        "        print(f\"✅ Installed missing packages: {missing}\")\n",
        "\n",
        "ensure_packages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqK9f0aHQtqb",
        "outputId": "39407770-34f5-4e4a-db69-0dbe7b12faa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-65278f7990b9>:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Installed missing packages: {'sentence_transformers'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File: data_ingestion/setup.py\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Sets up pandas and torch environment options for reproducibility and readability.\n",
        "    \"\"\"\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    torch.set_printoptions(sci_mode=False)\n",
        "    print(\"✅ Environment setup complete.\")\n",
        "\n",
        "\n",
        "def download_amazon_query_product_dataset(download_path=\"data/\"):\n",
        "    \"\"\"\n",
        "    Downloads the Amazon Query Product Search dataset from Kaggle using the Kaggle CLI.\n",
        "\n",
        "    Args:\n",
        "        download_path (str): The path where the dataset should be stored and unzipped.\n",
        "    \"\"\"\n",
        "    os.makedirs(download_path, exist_ok=True)\n",
        "    print(f\"📦 Downloading dataset to: {download_path}\")\n",
        "    subprocess.run([\n",
        "        \"kaggle\", \"datasets\", \"download\",\n",
        "        \"-d\", \"abhishekmungoli/amazon-query-product-search\",\n",
        "        \"-p\", download_path, \"--unzip\"\n",
        "    ])\n",
        "    print(\"✅ Dataset downloaded and unzipped.\")\n",
        "\n",
        "    # Log dataset location and hash to MLflow\n",
        "    dataset_file = os.path.join(download_path, \"train_v0.1.csv\")  # Example file, adjust as needed\n",
        "    if os.path.exists(dataset_file):\n",
        "        data_hash = hash_file(dataset_file)\n",
        "        mlflow.log_param(\"dataset_path\", dataset_file)\n",
        "        mlflow.log_param(\"data_hash\", data_hash)\n",
        "        mlflow.log_artifact(dataset_file, artifact_path=\"data\")\n",
        "        df = pd.read_csv(dataset_file)\n",
        "        mlflow.log_metric(\"total_rows\", len(df))\n",
        "        print(f\"🔐 Data hash logged: {data_hash}\")\n",
        "    else:\n",
        "        print(f\"⚠️ Warning: Expected dataset file not found at {dataset_file}\")\n",
        "\n",
        "\n",
        "def set_working_directory(path):\n",
        "    \"\"\"\n",
        "    Sets the working directory to the specified path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The path to switch to.\n",
        "    \"\"\"\n",
        "    if os.path.exists(path):\n",
        "        os.chdir(path)\n",
        "        print(f\"📂 Changed working directory to: {os.getcwd()}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Directory {path} does not exist\")\n",
        "\n",
        "\n",
        "def clean_memory():\n",
        "    \"\"\"\n",
        "    Frees up memory by collecting garbage and clearing CUDA cache.\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"🧹 Memory cleaned up.\")\n",
        "\n",
        "\n",
        "def hash_file(filepath, block_size=65536):\n",
        "    \"\"\"\n",
        "    Generate SHA256 hash of a file for version tracking.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the file to hash.\n",
        "        block_size (int): Block size to read in chunks.\n",
        "\n",
        "    Returns:\n",
        "        str: SHA256 hex digest of the file.\n",
        "    \"\"\"\n",
        "    sha256 = hashlib.sha256()\n",
        "    with open(filepath, 'rb') as f:\n",
        "        for block in iter(lambda: f.read(block_size), b''):\n",
        "            sha256.update(block)\n",
        "    return sha256.hexdigest()\n"
      ],
      "metadata": {
        "id": "eyaIewacag7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# === 🔧 Install Missing Packages ===\n",
        "def ensure_packages():\n",
        "    required = {'sentence_transformers', 'annoy', 'mlflow'}\n",
        "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "    missing = required - installed\n",
        "    if missing:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
        "        print(f\"✅ Installed missing packages: {missing}\")\n",
        "\n",
        "# === 🧠 Load Model ===\n",
        "def load_model(model_name='distilbert-base-uncased'):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"GPU available:\", torch.cuda.is_available())\n",
        "    return SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# === 🔄 Embedding Reshaper ===\n",
        "def reshape_array(input_array: np.ndarray, d: int) -> np.ndarray:\n",
        "    k, _ = input_array.shape\n",
        "    new_array = np.zeros((k, d))\n",
        "    for i in range(k):\n",
        "        for j in range(d):\n",
        "            start_idx = j * (768 // d)\n",
        "            end_idx = (j + 1) * (768 // d) if j < (d - 1) else 768\n",
        "            new_array[i, j] = np.mean(input_array[i, start_idx:end_idx])\n",
        "    return new_array\n",
        "\n",
        "# === 🔍 Sentence Embedding Function ===\n",
        "def find_embeddings(lst_product_title, i, step, model, product_dim, maxlen=None):\n",
        "    sentences = list(lst_product_title)[i:i + step]\n",
        "\n",
        "    sentences = list(lst_product_title)[i:i + step]\n",
        "\n",
        "    # Default maxlen: word count of first sentence\n",
        "    if maxlen is None and len(sentences) > 0:\n",
        "        maxlen = len(sentences[0].split())\n",
        "\n",
        "    # Truncate all sentences to the maxlen\n",
        "    sentences = [' '.join(s.split()[:maxlen]) for s in sentences]\n",
        "\n",
        "    embeddings = model.encode(sentences, convert_to_tensor=True).cpu().detach().numpy()\n",
        "    return reshape_array(embeddings, product_dim)\n"
      ],
      "metadata": {
        "id": "UIPJbu4_ajlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "\n",
        "def preprocess_product_search_data(\n",
        "    df_path='Dataset.csv',\n",
        "    products_path='shopping_queries_dataset_products.parquet',\n",
        "    positive_label='E',\n",
        "    sample_fraction=0.01,\n",
        "    random_state=42,\n",
        "    output_dir='outputs/'\n",
        "):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(df_path)\n",
        "    df_pos = df[df['esci_label'] == positive_label]\n",
        "    mlflow.log_metric(\"rows_positive\", len(df_pos))\n",
        "\n",
        "    # Sample subset of unique queries\n",
        "    unique_queries = df_pos['query'].unique()\n",
        "    sampled_queries = np.random.choice(unique_queries, int(len(unique_queries) * sample_fraction), replace=False)\n",
        "    df_sample = df_pos[df_pos['query'].isin(sampled_queries)].copy()\n",
        "    mlflow.log_param(\"sample_fraction\", sample_fraction)\n",
        "    mlflow.log_param(\"random_state\", random_state)\n",
        "    mlflow.log_metric(\"rows_sampled\", len(df_sample))\n",
        "    mlflow.log_metric(\"unique_queries\", df_sample['query'].nunique())\n",
        "\n",
        "    # Encode queries and product IDs\n",
        "    query_encoder = LabelEncoder()\n",
        "    product_encoder = LabelEncoder()\n",
        "    df_sample['queryid'] = query_encoder.fit_transform(df_sample['query'])\n",
        "    df_sample['productid'] = product_encoder.fit_transform(df_sample['product_id'])\n",
        "\n",
        "    joblib.dump(query_encoder, os.path.join(output_dir, \"query_encoder.pkl\"))\n",
        "    joblib.dump(product_encoder, os.path.join(output_dir, \"product_encoder.pkl\"))\n",
        "    mlflow.log_artifact(os.path.join(output_dir, \"query_encoder.pkl\"), artifact_path=\"encoders\")\n",
        "    mlflow.log_artifact(os.path.join(output_dir, \"product_encoder.pkl\"), artifact_path=\"encoders\")\n",
        "\n",
        "    # Load product metadata\n",
        "    df_products = pd.read_parquet(products_path)\n",
        "    product_cols = ['product_title', 'product_description', 'product_bullet_point', 'product_brand', 'product_color', 'product_id']\n",
        "    df_products = df_products[product_cols].drop_duplicates()\n",
        "    df_products = df_products.merge(df_sample[['product_id', 'productid']].drop_duplicates(), on='product_id', how='inner')\n",
        "\n",
        "    # Lowercase text columns\n",
        "    text_cols = ['product_title', 'product_description', 'product_bullet_point', 'product_brand', 'product_color']\n",
        "    for col in text_cols:\n",
        "        df_products[col] = df_products[col].apply(lambda x: str(x).lower() if pd.notna(x) else '')\n",
        "    df_sample['query'] = df_sample['query'].apply(lambda x: str(x).lower() if pd.notna(x) else '')\n",
        "\n",
        "    # Save and log outputs\n",
        "    df_sample.to_csv(os.path.join(output_dir, \"df_sample.csv\"), index=False)\n",
        "    df_products.to_csv(os.path.join(output_dir, \"df_products.csv\"), index=False)\n",
        "    mlflow.log_artifact(os.path.join(output_dir, \"df_sample.csv\"), artifact_path=\"processed\")\n",
        "    mlflow.log_artifact(os.path.join(output_dir, \"df_products.csv\"), artifact_path=\"processed\")\n",
        "\n",
        "    return df_sample, df_products\n"
      ],
      "metadata": {
        "id": "YMpvmANzf8rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "def embed_queries(queries_df, model, query_dim=32, step=10000, output_dir='outputs/query_embeddings'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cols = ['q' + str(x) for x in range(query_dim)] + ['query'] + ['query_id']\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(0, len(queries_df), step):\n",
        "        cnt += 1\n",
        "        sentences = queries_df['query'][i:i+step]\n",
        "        embeddings = model.encode(sentences, convert_to_tensor=True).cpu().numpy()\n",
        "        reshaped = reshape_array(embeddings, query_dim)\n",
        "\n",
        "        df_tmp = pd.DataFrame(np.concatenate(\n",
        "            [reshaped,\n",
        "             np.array(sentences).reshape(-1, 1),\n",
        "             np.array(queries_df['queryid'][i:i+step]).reshape(-1, 1)],\n",
        "            axis=1\n",
        "        ), columns=cols)\n",
        "\n",
        "        path = os.path.join(output_dir, f'query_{cnt}.parquet')\n",
        "        df_tmp.to_parquet(path, index=False)\n",
        "        mlflow.log_artifact(path, artifact_path='query_embeddings')\n",
        "        print(f\"✅ Saved query batch {cnt}: {path}\")\n",
        "\n",
        "def embed_products(df_products, model, product_dim=32, step=1000, output_dir='outputs/product_embeddings'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cols = ['p' + str(i) for i in range(product_dim * 5)] + ['product_id', 'productid']\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(0, len(df_products), step):\n",
        "        cnt += 1\n",
        "        embeds = []\n",
        "\n",
        "        for col, maxlen in zip(\n",
        "            ['product_title', 'product_description', 'product_bullet_point', 'product_brand', 'product_color'],\n",
        "            [50, 20, 50, 5, 5]\n",
        "        ):\n",
        "            df_products[col] = df_products[col].apply(lambda x: str(x).lower() if pd.notna(x) else '')\n",
        "            embed = find_embeddings(list(df_products[col]), i, step, model, product_dim, maxlen)\n",
        "            embeds.append(embed)\n",
        "\n",
        "        df_tmp = pd.DataFrame(np.concatenate(embeds + [\n",
        "            np.array(df_products['product_id'][i:i+step]).reshape(-1, 1),\n",
        "            np.array(df_products['productid'][i:i+step]).reshape(-1, 1)\n",
        "        ], axis=1), columns=cols)\n",
        "\n",
        "        path = os.path.join(output_dir, f'product_{cnt}.parquet')\n",
        "        df_tmp.to_parquet(path, index=False)\n",
        "        mlflow.log_artifact(path, artifact_path='product_embeddings')\n",
        "        print(f\"✅ Saved product batch {cnt}: {path}\")"
      ],
      "metadata": {
        "id": "C_uSuHa0nvTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "import mlflow\n",
        "\n",
        "def build_annoy_index(embeddings: np.ndarray, metric: str = 'angular', n_trees: int = 20):\n",
        "    index = AnnoyIndex(embeddings.shape[1], metric)\n",
        "    for i, vector in enumerate(embeddings):\n",
        "        index.add_item(i, vector)\n",
        "    index.build(n_trees)\n",
        "    print(f\"✅ Annoy index built with {n_trees} trees\")\n",
        "    return index\n",
        "\n",
        "def get_similar_candidates(index, embeddings: np.ndarray, top_k: int = 100):\n",
        "    similar_candidates = []\n",
        "    for vector in embeddings:\n",
        "        indices, distances = index.get_nns_by_vector(vector, top_k, include_distances=True)\n",
        "        indices, distances = indices[1:], distances[1:]  # skip self\n",
        "        similar_candidates.append(list(zip(indices, distances)))\n",
        "    return similar_candidates\n",
        "\n",
        "def construct_triplets(df_pos: pd.DataFrame, similar_candidates: list):\n",
        "    triplets = []\n",
        "    for user_id, group in df_pos.groupby('queryid'):\n",
        "        pos_items = group['productid'].tolist()\n",
        "        for pos_item in pos_items:\n",
        "            candidates = similar_candidates[pos_item]\n",
        "            negatives = [cand[0] for cand in candidates if cand[0] not in pos_items]\n",
        "            triplets.append((user_id, pos_item, negatives))\n",
        "    return pd.DataFrame(triplets, columns=['query_id', 'product_id', 'negative_samples'])\n",
        "\n",
        "def save_triplets(triplets_df: pd.DataFrame, path: str = \"outputs/triplets.parquet\"):\n",
        "    triplets_df.to_parquet(path, index=False)\n",
        "    mlflow.log_artifact(path, artifact_path=\"triplet_data\")\n",
        "    print(f\"✅ Triplets saved and logged: {path}\")"
      ],
      "metadata": {
        "id": "xqLEmbAFoHEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source: current working directory\n",
        "os.chdir('/content/drive/MyDrive/colab_backup')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFY5vj6WS5fW",
        "outputId": "1511fd54-911a-42fe-8e58-5036bdfa90d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import mlflow\n",
        "# # import pandas as pd\n",
        "# # import torch\n",
        "# # import os\n",
        "# # import numpy as np\n",
        "# # from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # from data_ingestion.setup import setup_environment, download_amazon_query_product_dataset\n",
        "# # from preprocess_product_search_data import preprocess_product_search_data\n",
        "# # from features.generate_embeddings import embed_queries, embed_products\n",
        "# # from features.generate_triplets import build_annoy_index, get_similar_candidates, construct_triplets, save_triplets\n",
        "\n",
        "\n",
        "# # === 🔧 SETUP ===\n",
        "# mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "# mlflow.set_experiment(\"amazon_query_product_retrieval\")\n",
        "\n",
        "# with mlflow.start_run():\n",
        "#     # Step 1: Environment setup + download raw data\n",
        "#     # setup_environment()\n",
        "#     # # download_amazon_query_product_dataset(\"data/\")\n",
        "\n",
        "#     # # Step 2: Preprocess data\n",
        "#     # df_sample, df_products = preprocess_product_search_data(\n",
        "#     #     df_path=\"Dataset.csv\",\n",
        "#     #     products_path=\"shopping_queries_dataset_products.parquet\",\n",
        "#     #     sample_fraction=0.1,\n",
        "#     #     random_state=42,\n",
        "#     #     output_dir=\"outputs/\"\n",
        "#     # )\n",
        "\n",
        "#     # # Step 3: Generate Embeddings\n",
        "#     # model = SentenceTransformer('distilbert-base-uncased', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     # queries_df = df_sample[['query', 'queryid']].drop_duplicates().reset_index(drop=True)\n",
        "#     # embed_queries(queries_df=queries_df, model=model, query_dim=32, step=10000, output_dir='outputs/query_embeddings')\n",
        "#     # embed_products(df_products=df_products, model=model, product_dim=32, step=1000, output_dir='outputs/product_embeddings')\n",
        "\n",
        "# # Step 4: Load concatenated embeddings\n",
        "#     # query_embeds = pd.concat([pd.read_parquet(f'outputs/query_embeddings/{f}') for f in os.listdir('outputs/query_embeddings')])\n",
        "#     # product_embeds = pd.concat([pd.read_parquet(f'outputs/product_embeddings/{f}') for f in os.listdir('outputs/product_embeddings')])\n",
        "\n",
        "#     # query_embeddings = query_embeds.drop(columns=['query', 'query_id']).values\n",
        "#     # product_embeddings = product_embeds.drop(columns=['product_id', 'productid']).values\n",
        "\n",
        "#     # # Step 5: Build index + sample negatives\n",
        "#     # annoy_index = build_annoy_index(product_embeddings)\n",
        "#     # similar_candidates = get_similar_candidates(annoy_index, product_embeddings)\n",
        "\n",
        "#     # triplets_df = construct_triplets(df_sample, similar_candidates)\n",
        "#     save_triplets(triplets_df)"
      ],
      "metadata": {
        "id": "dzibjwexoowN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import pytorch_lightning as pl\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
        "# from collections import defaultdict\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import random\n",
        "\n",
        "\n",
        "# class RecommendationModel(pl.LightningModule):\n",
        "#     def __init__(self, query_embeddings, product_embeddings, learning_rate=1e-3):\n",
        "#         super().__init__()\n",
        "#         self.query_embeddings = nn.Embedding.from_pretrained(\n",
        "#             torch.tensor(query_embeddings, dtype=torch.float32), freeze=True\n",
        "#         )\n",
        "#         self.product_embeddings = nn.Embedding.from_pretrained(\n",
        "#             torch.tensor(product_embeddings, dtype=torch.float32), freeze=True\n",
        "#         )\n",
        "#         self.fc = nn.Linear(query_embeddings.shape[1] + product_embeddings.shape[1], 1)\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.validation_step_outputs = []\n",
        "#         self.test_step_outputs = []\n",
        "#         self.save_hyperparameters()\n",
        "\n",
        "#     def forward(self, query_ids, product_ids):\n",
        "#         query_emb = self.query_embeddings(query_ids)\n",
        "#         product_emb = self.product_embeddings(product_ids)\n",
        "#         combined = torch.cat((query_emb, product_emb), dim=1)\n",
        "#         return self.fc(combined).squeeze()\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         user_ids, product_ids, labels = batch\n",
        "#         predictions = self(user_ids, product_ids)\n",
        "#         loss = nn.BCEWithLogitsLoss()(predictions, labels.float())\n",
        "#         self.log('train_loss', loss)\n",
        "#         return loss\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         user_ids, product_ids, labels = batch\n",
        "#         logits = self(user_ids, product_ids)\n",
        "#         probs = torch.sigmoid(logits)\n",
        "#         loss = nn.BCEWithLogitsLoss()(logits, labels.float())\n",
        "#         self.log('val_loss', loss)\n",
        "\n",
        "#         self.validation_step_outputs.append({\n",
        "#             'user_ids': user_ids.detach().cpu(),\n",
        "#             'product_ids': product_ids.detach().cpu(),\n",
        "#             'labels': labels.detach().cpu(),\n",
        "#             'probs': probs.detach().cpu()\n",
        "#         })\n",
        "\n",
        "#     def on_validation_epoch_end(self):\n",
        "#         self._log_epoch_metrics(self.validation_step_outputs, prefix=\"Val\")\n",
        "#         self.validation_step_outputs.clear()\n",
        "\n",
        "#     def test_step(self, batch, batch_idx):\n",
        "#         user_ids, product_ids, labels = batch\n",
        "#         logits = self(user_ids, product_ids)\n",
        "#         probs = torch.sigmoid(logits)\n",
        "#         self.test_step_outputs.append({\n",
        "#             'user_ids': user_ids.detach().cpu(),\n",
        "#             'product_ids': product_ids.detach().cpu(),\n",
        "#             'labels': labels.detach().cpu(),\n",
        "#             'probs': probs.detach().cpu()\n",
        "#         })\n",
        "\n",
        "#     def on_test_epoch_end(self):\n",
        "#         self._log_epoch_metrics(self.test_step_outputs, prefix=\"Test\")\n",
        "#         self.test_step_outputs.clear()\n",
        "\n",
        "#     def _log_epoch_metrics(self, outputs, prefix=\"\"):\n",
        "#         user_pred_dict = defaultdict(list)\n",
        "#         user_true_dict = defaultdict(set)\n",
        "#         for output in outputs:\n",
        "#             for uid, pid, label, score in zip(output['user_ids'], output['product_ids'], output['labels'], output['probs']):\n",
        "#                 uid, pid = int(uid), int(pid)\n",
        "#                 user_pred_dict[uid].append((pid, score))\n",
        "#                 if label == 1:\n",
        "#                     user_true_dict[uid].add(pid)\n",
        "#         y_true_grouped = []\n",
        "#         y_pred_grouped = []\n",
        "#         for user in user_pred_dict:\n",
        "#             sorted_preds = sorted(user_pred_dict[user], key=lambda x: x[1], reverse=True)\n",
        "#             y_pred_grouped.append([pid for pid, _ in sorted_preds])\n",
        "#             y_true_grouped.append(list(user_true_dict[user]))\n",
        "#         metrics = calculate_metrics(y_true_grouped, y_pred_grouped, k_values=[5, 10, 15])\n",
        "#         for key, value in metrics.items():\n",
        "#             self.log(f\"{prefix}/{key}\", value, prog_bar=True)\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "#     def predict_from_embedding(self, query_emb_float, product_emb_matrix):\n",
        "#         \"\"\"\n",
        "#         Predict top-k scores from float query embedding and full product embedding matrix.\n",
        "\n",
        "#         Args:\n",
        "#             query_emb_float: torch.Tensor of shape [1, d]\n",
        "#             product_emb_matrix: torch.Tensor of shape [N, d]\n",
        "\n",
        "#         Returns:\n",
        "#             scores: torch.Tensor of shape [N]\n",
        "#         \"\"\"\n",
        "#         with torch.no_grad():\n",
        "#             repeated_query = query_emb_float.repeat(product_emb_matrix.shape[0], 1)\n",
        "#             combined = torch.cat((repeated_query, product_emb_matrix), dim=1)\n",
        "#             logits = self.fc(combined).squeeze()\n",
        "#             return torch.sigmoid(logits)\n",
        "\n",
        "\n",
        "# def calculate_metrics(y_true, y_pred, k_values=[5, 10, 15]):\n",
        "#     results = {}\n",
        "#     for k in k_values:\n",
        "#         y_pred_truncated = [pred[:k] if len(pred) >= k else pred + [0] * (k - len(pred)) for pred in y_pred]\n",
        "#         y_true_truncated = [true[:k] if len(true) >= k else true + [0] * (k - len(true)) for true in y_true]\n",
        "#         try:\n",
        "#             results[f'NDCG_{k}'] = ndcg_score(np.array([y_true_truncated]), np.array([y_pred_truncated]))\n",
        "#         except ValueError:\n",
        "#             results[f'NDCG_{k}'] = 0\n",
        "#         try:\n",
        "#             results[f'Precision_{k}'] = precision_score(np.array(y_true_truncated).flatten(),\n",
        "#                                                         np.array(y_pred_truncated).flatten(),\n",
        "#                                                         average='macro', zero_division=0)\n",
        "#         except ValueError:\n",
        "#             results[f'Precision_{k}'] = 0\n",
        "#         try:\n",
        "#             results[f'Recall_{k}'] = recall_score(np.array(y_true_truncated).flatten(),\n",
        "#                                                   np.array(y_pred_truncated).flatten(),\n",
        "#                                                   average='macro', zero_division=0)\n",
        "#         except ValueError:\n",
        "#             results[f'Recall_{k}'] = 0\n",
        "#     return results\n",
        "\n",
        "\n",
        "# def explode_triplets(triplets, negatives):\n",
        "#     exploded_triplets = []\n",
        "#     for user_id, pos_items, neg_items in triplets:\n",
        "#         pos_items = [pos_items] if isinstance(pos_items, int) else pos_items\n",
        "#         for pos_item in pos_items:\n",
        "#             exploded_triplets.append([user_id, pos_item, 1])\n",
        "#         for neg_item in random.sample(list(neg_items), negatives):\n",
        "#             exploded_triplets.append([user_id, neg_item, 0])\n",
        "#     return exploded_triplets\n",
        "\n",
        "\n",
        "# def train_val_test_split(triplets, negatives, train_size=0.8, val_size=0.1, test_size=0.1, random_state=42):\n",
        "#     exploded_data = explode_triplets(triplets, negatives)\n",
        "#     df_exploded = pd.DataFrame(exploded_data, columns=['user_id', 'item_id', 'label'])\n",
        "#     train_data_df, temp_data_df = train_test_split(df_exploded, train_size=train_size, random_state=random_state)\n",
        "#     val_data_df, test_data_df = train_test_split(temp_data_df, train_size=val_size/(val_size + test_size), random_state=random_state)\n",
        "#     train_data = train_data_df.values.tolist()\n",
        "#     val_data = val_data_df.values.tolist()\n",
        "#     test_data = test_data_df.values.tolist()\n",
        "#     return train_data, val_data, test_data\n"
      ],
      "metadata": {
        "id": "VtQ_vtAUogsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import sys\n",
        "# import mlflow\n",
        "# import joblib\n",
        "# import torch\n",
        "# import pandas as pd\n",
        "# from itertools import product\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from pytorch_lightning import Trainer\n",
        "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "# from pytorch_lightning.loggers import TensorBoardLogger\n",
        "# # from recommendation_model import RecommendationModel, train_val_test_split, calculate_metrics\n",
        "\n",
        "\n",
        "# def install_dependencies():\n",
        "#     import subprocess\n",
        "#     import pkg_resources\n",
        "#     required = {'mlflow', 'pytorch-lightning'}\n",
        "#     installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "#     missing = required - installed\n",
        "#     if missing:\n",
        "#         python = sys.executable\n",
        "#         subprocess.check_call([python, '-m', 'pip', 'install', *missing])\n",
        "\n",
        "\n",
        "# def make_loader(data, batch_size=128):\n",
        "#     return DataLoader(TensorDataset(\n",
        "#         torch.tensor([x[0] for x in data]),\n",
        "#         torch.tensor([x[1] for x in data]),\n",
        "#         torch.tensor([x[2] for x in data])\n",
        "#     ), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# def run_training_pipeline():\n",
        "#     # Load embeddings and triplets\n",
        "#     df_triplets = pd.read_parquet('outputs/triplets.parquet')\n",
        "#     query_embeds = pd.concat([pd.read_parquet(f'outputs/query_embeddings/{f}') for f in os.listdir('outputs/query_embeddings')])\n",
        "#     product_embeds = pd.concat([pd.read_parquet(f'outputs/product_embeddings/{f}') for f in os.listdir('outputs/product_embeddings')])\n",
        "\n",
        "#     product_embeddings = product_embeds.sort_values(by='product_id').drop(columns=['product_id', 'productid']).values\n",
        "#     query_embeddings = query_embeds.sort_values(by='query_id').drop(columns=['query_id', 'query']).values\n",
        "\n",
        "#     # MLflow setup\n",
        "#     mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "#     mlflow.set_experiment(\"my_recommendation_experiment\")\n",
        "\n",
        "#     negative_samples_list = [1, 10, 30]\n",
        "#     learning_rates = [1e-3]\n",
        "#     max_epoch = 5\n",
        "#     batch_size = 256\n",
        "\n",
        "#     for negative_samples, lr in product(negative_samples_list, learning_rates):\n",
        "#         run_name = f\"neg{negative_samples}_lr{lr}\"\n",
        "\n",
        "#         with mlflow.start_run(run_name=run_name):\n",
        "#             mlflow.log_param(\"num_negative_samples\", negative_samples)\n",
        "#             mlflow.log_param(\"learning_rate\", lr)\n",
        "#             mlflow.log_param(\"max_epochs\", max_epoch)\n",
        "#             mlflow.log_param(\"data_version\", \"hard_negative_22042025\")\n",
        "\n",
        "#             train_data, val_data, test_data = train_val_test_split(\n",
        "#                 df_triplets.values,\n",
        "#                 negative_samples,\n",
        "#                 train_size=0.8,\n",
        "#                 val_size=0.1,\n",
        "#                 test_size=0.1,\n",
        "#                 random_state=42\n",
        "#             )\n",
        "\n",
        "#             train_loader = make_loader(train_data, batch_size)\n",
        "#             val_loader = make_loader(val_data, batch_size)\n",
        "#             test_loader = make_loader(test_data, batch_size)\n",
        "\n",
        "#             model = RecommendationModel(\n",
        "#                 query_embeddings=query_embeddings,\n",
        "#                 product_embeddings=product_embeddings,\n",
        "#                 learning_rate=lr\n",
        "#             )\n",
        "\n",
        "#             tb_logger = TensorBoardLogger(\"lightning_logs\", name=run_name)\n",
        "#             checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n",
        "\n",
        "#             trainer = Trainer(\n",
        "#                 max_epochs=max_epoch,\n",
        "#                 logger=tb_logger,\n",
        "#                 callbacks=[checkpoint_callback],\n",
        "#                 enable_model_summary=True\n",
        "#             )\n",
        "\n",
        "#             trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "#             mlflow.pytorch.log_model(model, artifact_path=\"rec_model\")\n",
        "\n",
        "#             results = trainer.test(model, dataloaders=test_loader)\n",
        "#             for metric, value in results[0].items():\n",
        "#                 mlflow.log_metric(metric, value)\n",
        "\n",
        "#             # Register the model\n",
        "#             model_uri = f\"runs:/{mlflow.active_run().info.run_id}/rec_model\"\n",
        "#             model_name = \"RecommendationModel\"\n",
        "#             try:\n",
        "#                 mlflow.register_model(model_uri, model_name)\n",
        "#             except Exception:\n",
        "#                 pass\n",
        "\n",
        "#             from mlflow.tracking import MlflowClient\n",
        "#             client = MlflowClient()\n",
        "#             model_versions = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
        "#             new_version = client.get_latest_versions(model_name, stages=[\"None\"])[0].version\n",
        "\n",
        "#             # Get metric to compare\n",
        "#             new_score = results[0].get(\"Test/Recall_5\", 0)\n",
        "#             if model_versions:\n",
        "#                 old_version = model_versions[0].version\n",
        "#                 old_score = float(client.get_model_version(model_name, old_version).tags.get(\"Test/Recall_5\", 0))\n",
        "#                 if new_score > old_score:\n",
        "#                     client.transition_model_version_stage(model_name, new_version, stage=\"Production\")\n",
        "#                     client.transition_model_version_stage(model_name, old_version, stage=\"Archived\")\n",
        "#             else:\n",
        "#                 client.transition_model_version_stage(model_name, new_version, stage=\"Production\")\n",
        "#             client.set_model_version_tag(model_name, new_version, \"Test/Recall_5\", str(new_score))\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # install_dependencies()\n",
        "#     run_training_pipeline()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t9G8EWqmyfU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlflow\n",
        "\n",
        "# client = mlflow.tracking.MlflowClient()\n",
        "# model_name = \"RecommendationModel\"\n",
        "\n",
        "# # Get all versions\n",
        "# versions = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
        "\n",
        "# # Demote each Production version (optional: move to Archived)\n",
        "# for v in versions:\n",
        "#     client.transition_model_version_stage(\n",
        "#         name=model_name,\n",
        "#         version=v.version,\n",
        "#         stage=\"Archived\"  # or \"Staging\"\n",
        "#     )\n",
        "#     print(f\"Version {v.version} transitioned to Archived.\")"
      ],
      "metadata": {
        "id": "y6VbTMw41b2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow.pytorch\n",
        "import joblib\n",
        "from threading import Thread\n",
        "import os\n",
        "\n",
        "# --- Load everything once ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "rec_model = mlflow.pytorch.load_model(\"models:/RecommendationModel/Production\").to(device)\n",
        "rec_model.eval()\n",
        "\n",
        "query_encoder_model = SentenceTransformer('distilbert-base-uncased', device=device)\n",
        "\n",
        "product_embedding_df = pd.concat([pd.read_parquet(\n",
        "    f'outputs/product_embeddings/{f}') for f in os.listdir(\n",
        "        'outputs/product_embeddings')])\n",
        "\n",
        "product_ids = product_embedding_df.sort_values(by='product_id')[['product_id', 'productid']]\n",
        "product_embeddings = product_embedding_df.sort_values(by='product_id').drop(\n",
        "            columns=['product_id', 'productid']).values\n",
        "\n",
        "product_emb_tensor = torch.tensor(product_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "products_df = pd.read_csv('outputs/df_products.csv')\n",
        "product_encoder = joblib.load('outputs/product_encoder.pkl')\n",
        "productid_to_metadata = products_df.set_index('productid')[['product_id', 'product_title']]\n",
        "index_to_productid = product_ids.reset_index(drop=True)['productid'].values\n",
        "\n",
        "query_dim = 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orNz-BL360QV",
        "outputId": "3e6963a8-7ba8-4dde-b7c2-3745585215c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-uncased. Creating a new one with mean pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Flask app ---\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({'status': 'ok'})\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        query_text = data.get('query')\n",
        "        if not query_text:\n",
        "            return jsonify({'error': 'No query provided'}), 400\n",
        "\n",
        "        query_emb = query_encoder_model.encode([query_text], convert_to_tensor=True).to(device)\n",
        "        query_emb = reshape_array(query_emb.cpu().numpy(), query_dim)\n",
        "        query_emb = torch.tensor(query_emb, dtype=torch.float32).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            scores = rec_model.predict_from_embedding(query_emb, product_emb_tensor).squeeze()\n",
        "            scores = torch.sigmoid(scores)\n",
        "\n",
        "        topk = 5\n",
        "        top_indices = torch.topk(scores, topk).indices.cpu().numpy()\n",
        "        top_scores = torch.topk(scores, topk).values.cpu().numpy()\n",
        "\n",
        "        top_productids = index_to_productid[top_indices]\n",
        "        original_product_ids = product_encoder.inverse_transform(top_productids)\n",
        "\n",
        "        results = productid_to_metadata.loc[top_productids].copy()\n",
        "        results['real_product_id'] = original_product_ids\n",
        "        results['score'] = top_scores\n",
        "\n",
        "        return jsonify(results.reset_index(drop=True).to_dict(orient='records'))\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        tb = traceback.format_exc()\n",
        "\n",
        "        debug_info = {\n",
        "            \"error\": str(e),\n",
        "            \"traceback\": tb,\n",
        "            \"query_emb_shape\": list(query_emb.shape) if 'query_emb' in locals() else \"undefined\",\n",
        "            \"product_emb_tensor_shape\": list(product_emb_tensor.shape) if 'product_emb_tensor' in locals() else \"undefined\",\n",
        "            \"fc_input_dim\": list(rec_model.fc.weight.shape) if hasattr(rec_model, 'fc') else \"undefined\"\n",
        "        }\n",
        "\n",
        "        return jsonify(debug_info), 500\n",
        "\n",
        "@app.route('/batch_predict', methods=['POST'])\n",
        "def batch_predict():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        queries = data.get('queries')\n",
        "\n",
        "        if not queries or not isinstance(queries, list):\n",
        "            return jsonify({'error': 'No valid queries list provided'}), 400\n",
        "\n",
        "        batch_results = []\n",
        "\n",
        "        for query_text in queries:\n",
        "            query_emb = query_encoder_model.encode([query_text], convert_to_tensor=True).to(device).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                scores = rec_model.predict_from_embedding(query_emb, product_emb_tensor).squeeze()\n",
        "                scores = torch.sigmoid(scores)\n",
        "\n",
        "            topk = 5\n",
        "            top_indices = torch.topk(scores, topk).indices.cpu().numpy()\n",
        "            top_scores = torch.topk(scores, topk).values.cpu().numpy()\n",
        "\n",
        "            top_productids = index_to_productid[top_indices]\n",
        "            original_product_ids = product_encoder.inverse_transform(top_productids)\n",
        "\n",
        "            results = productid_to_metadata.loc[top_productids].copy()\n",
        "            results['real_product_id'] = original_product_ids\n",
        "            results['score'] = top_scores\n",
        "\n",
        "            batch_results.append({\n",
        "                'query': query_text,\n",
        "                'results': results.reset_index(drop=True).to_dict(orient='records')\n",
        "            })\n",
        "\n",
        "        return jsonify(batch_results)\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "def run_server():\n",
        "    print(\"🟢 Starting Flask server...\")\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=True, use_reloader=False)"
      ],
      "metadata": {
        "id": "8qGM9NNukfGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "server_thread = Thread(target=run_server)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2atFzZD4ucTB",
        "outputId": "03b88ddb-76c6-4954-93e8-a2b33dfbbdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 Starting Flask server...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:5000/health"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGjlB6gx5yPl",
        "outputId": "2d852f4c-377c-4817-f67b-43660e854c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Apr/2025 12:39:50] \"GET /health HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"ok\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.post(\"http://localhost:5000/predict\", json={\"query\": \"wireless earbuds\"})\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GUELOhQuqyv",
        "outputId": "2c5603fb-4d12-447d-fc98-7e25997403b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Apr/2025 12:39:51] \"\u001b[35m\u001b[1mPOST /predict HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error': 'Length of values (5) does not match length of index (6)', 'fc_input_dim': [1, 192], 'product_emb_tensor_shape': 'undefined', 'query_emb_shape': [1, 32], 'traceback': 'Traceback (most recent call last):\\n  File \"<ipython-input-13-d4d07a3840bd>\", line 32, in predict\\n    results[\\'real_product_id\\'] = original_product_ids\\n    ~~~~~~~^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 4311, in __setitem__\\n    self._set_item(key, value)\\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 4524, in _set_item\\n    value, refs = self._sanitize_column(value)\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 5266, in _sanitize_column\\n    com.require_length_match(value, self.index)\\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\", line 573, in require_length_match\\n    raise ValueError(\\nValueError: Length of values (5) does not match length of index (6)\\n'}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}